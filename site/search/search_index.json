{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NovaScope documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>NovaScope is a Snakemake-based pipeline that processes spatial transcriptomics data generated from the Seq-Scope. Currently, it is tailored to process the spatial arrays generated from the Illumina NovaSeq 6000 platform.</p> <p>The pipeline is designed to process raw sequencing data (1st-seq and 2nd-seq), align reads to the reference genome, and produce spatial gene expression at the submicron resolution. The pipeline is designed to be modular and flexible, allowing users to customize the pipeline to their specific needs. </p> <p>The pipeline is designed to be run on a Unix-based high-performance computing (HPC) system, either locally or through the Slurm workload manager.</p> <p>NovaScope consists of primarily two steps as shown in the figure below.</p> <p>Figure 1: Overview of the NovaScope pipeline: Step 1 processes the 1st-seq FASTQ files to generate spatial barcode maps for each \"Chip\", a 10x6 array of tiles. Step 2 processes the 2nd-seq FASTQ files, aligns reads to the reference genome, and produces spatial gene expression at submicron resolution.  </p>"},{"location":"getting_started/access_data/","title":"Accessing Example Datasets","text":""},{"location":"getting_started/access_data/#dataset-overview","title":"Dataset Overview","text":"<p>There are three example datasets published with the NovaScope protocol. Each input dataset contains two types of FASTQ files: (a) 1st-seq (single-end) FASTQ file that contains spatial barcodes to construct a barcode map, and (b) 2nd-seq (paired-end) FASTQ files that contains spatial barcodes in Read 1, and cDNA sequences in Read 2. </p>"},{"location":"getting_started/access_data/#minimal-test-run-dataset","title":"Minimal Test Run Dataset","text":"<p>This is a small (1.14GB) test run dataset comprising of a subset of the (shallow) liver section data described in Shallow Liver Section Dataset. This dataset is meant to be used to test the sanity of the pipeline, without necessarily offering biologically meaningful interpretation of data. </p>"},{"location":"getting_started/access_data/#shallow-liver-section-dataset","title":"Shallow Liver Section Dataset","text":"<p>This dataset is a typical (**GB) example of Seq-Scope dataset that can be initially generated for a tissue section. Typically, the 2nd-seq FASTQ files contain 150-200M paired-end reads. This should be sufficient to examine the spatial distribution of the transcripts across the tissue, assess the quality of dataset, identify major cell types and marker genes, and perform basic pixel-level decoding of the spatial transcriptome. If the quality of the initial dataset look great, one may decide to sequence the library much more deeply to maximize the information content. (see Deep Liver Section Dataset for more details)</p>"},{"location":"getting_started/access_data/#deep-liver-section-dataset","title":"Deep Liver Section Dataset","text":"<p>If the initial examination of the shallow dataset looks promising, one can sequence the library much more deeply, to the level of saturating the library. This dataset typically contains multiple pairs of FASTQ files, possibly across multiple sequencing runs. </p>"},{"location":"getting_started/access_data/#downloading-the-datasets","title":"Downloading the Datasets","text":"<p>Each of the three datasets have their own DOIs, which can be accessed using the URLs below.</p> <ul> <li>Minimal Test Run Dataset : https://doi.org/10.5281/zenodo.10835761</li> </ul> <pre><code>## To download the tarball from Zenodo, you can use the following command\nwget \"https://zenodo.org/records/10835761/files/B08Csub_20240318_raw.tar.gz\"\n\n## uncompress the tarball using the following command:\nmkdir B08Ctest\ncd B08Ctest\ntar xzvf ../B08Csub_20240318_raw.tar.gz\n</code></pre> <ul> <li>Shallow Liver Section Dataset : https://doi.org/10.5281/zenodo.10840696 </li> </ul> <pre><code>## To download the tarball from Zenodo, you can use the following command\n\n## create a directory to store the data\nmkdir B08Cshallow\ncd B08Cshallow\n\n## download the 1st-seq FASTQ file\nwget \"https://zenodo.org/records/10840696/files/9203-AP.L3.B08C.R1_001.fastq.gz\"\n\n## download the 2nd-seq FASTQ files (R1 and R2)\nwget \"https://zenodo.org/records/10840696/files/9748-YK-3_CGAGGCTG_S3_R1_001.fastq.gz\"\nwget \"https://zenodo.org/records/10840696/files/9748-YK-3_CGAGGCTG_S3_R2_001.fastq.gz\"\n\n## Additionally, you may want to download md5sum files \n## to verify the integrity of the downloaded files\n</code></pre> <ul> <li>Deep Liver Section Dataset : Link to Deep Blue Data </li> <li>Note that this dataset contains only additional 2nd-seq FASTQ files in addition to the Shallow Liver Section Dataset. </li> </ul> <p>Using the above URL, you can download additional 2nd-seq FASTQ files, either using browser or Globus.</p>"},{"location":"getting_started/execute/","title":"Executing the NovaScope Pipeline","text":""},{"location":"getting_started/execute/#preliminary-steps","title":"Preliminary Steps","text":"<p>Before running the full pipeline, performing a sanity check by executing a dry run is highly recommended. A dry run verifies that your <code>config_job.yaml</code> is properly configured and outlines the necessary jobs to be executed. </p> <p>Additionally, you can create a rule graph that visually represents the structure of the workflow or a DAG (Directed Acyclic Graph) to view all jobs and their actual dependency structure.</p> <pre><code># Paths\nsmk_dir=\"&lt;path_to_NovaScope_repository&gt;\"    # Replace &lt;path_to_NovaScope_repository&gt; with the path to the NovaScope repository\njob_dir=\"&lt;job_directory&gt;\"                   # Replace &lt;job_directory&gt; with your specific job directory path, which has the `config_job.yaml` file.\n\n## (Recommended) Start with a dry run\n## - View all information:\nsnakemake -s $smk_dir/NovaScope.smk --rerun-incomplete -d $job_dir --dry-run -p\n\n## - Simply summarize the jobs to be executed without other information:\nsnakemake -s $smk_dir/NovaScope.smk --rerun-incomplete -d $job_dir --dry-run --quiet\n\n## (Optional) Visualization.\n## - (1) Rulegraph\nsnakemake --rulegraph  -s $smk_dir/NovaScope.smk --rerun-incomplete -d $job_dir | dot -Tpdf &gt; rulegraph.pdf\n\n## - (2) DAG\nsnakemake --dag  -s $smk_dir/NovaScope.smk --rerun-incomplete -d $job_dir | dot -Tpdf &gt; dag.pdf\n</code></pre>"},{"location":"getting_started/execute/#execution-options","title":"Execution Options","text":""},{"location":"getting_started/execute/#option-a-local-execution","title":"Option A: Local Execution","text":"<p>If your computing environment does not require a job scheduler such as Slurm, you can run the pipeline locally. You will need to specify the number of cores.</p> <p>An example script is provided below. Make sure to replace the variables to relevant paths and the number of cores.</p> <pre><code>smk_dir=\"&lt;path_to_NovaScope_repository&gt;\"  # Replace &lt;path_to_NovaScope_repository&gt; with the path to the NovaScope repository\njob_dir=\"&lt;path_to_the_job_directory&gt;\"     # Replace &lt;job_directory&gt; with your specific job directory path that contains the `config_job.yaml` file\n\nNcores=8                                  # Replace to the number of available CPU cores you wish to use\n\nsnakemake --latency-wait 120 -s ${smk_dir}/NovaScope.smk -d $job_dir --cores $Ncores --rerun-incomplete \n</code></pre> <p>See the following examples to see how to execute the pipeline locally:</p> <ul> <li>Minimal Test Run</li> <li>Shallow Liver Section</li> <li>Deep Liver Section</li> </ul>"},{"location":"getting_started/execute/#option-b-slurm-using-a-master-job","title":"Option B: Slurm using a Master Job","text":"<p>If your computing environment expects to run jobs via a job scheduler such Slurm, a recommended approach to submit a 'Master Job' that oversees and manage the status of all other jobs. </p> <p>First, you need to establish the master job. The primary role of this job is to monitor the progress of all tasks, handle job submissions based on dependencies and available resources. Thus, it requires minimal memory but an extended time limit. Its time limit should be longer than the total time required to complete all associated jobs.</p> <p>Create a file similar to the information below. Note that the details of the contents may vary based on your specific computing environment. </p> <pre><code>#!/bin/bash\n####  Job configuration\n#SBATCH --account=&lt;account_name&gt;               # Replace &lt;account_name&gt; with your account identifier\n#SBATCH --partition=&lt;partition_name&gt;           # Replace &lt;partition_name&gt; with your partition name\n#SBATCH --job-name=&lt;job_name&gt;                  # Replace &lt;job_name&gt; with a name for your job\n#SBATCH --nodes=1                              # Number of nodes, adjust as needed\n#SBATCH --ntasks-per-node=1                    # Number of tasks per node, adjust based on requirement\n#SBATCH --cpus-per-task=1                      # Number of CPUs per task, adjust as needed\n#SBATCH --mem-per-cpu=&lt;memory_allocation&gt;      # Memory per CPU, replace &lt;memory_allocation&gt; with value, e.g., \"2000m\"\n#SBATCH --time=&lt;time_limit&gt;                    # Job time limit, replace &lt;time_limit&gt; with value, e.g., \"72:00:00\"\n#SBATCH --mail-user=&lt;your_email&gt;               # Replace &lt;your_email&gt; with your email address\n#SBATCH --mail-type=END,FAIL,REQUEUE           # Notification types for job status\n#SBATCH --output=./logs/&lt;log_filename&gt;         # Replace &lt;log_filename&gt; with the log file name pattern\n\n# Paths\nsmk_dir=\"&lt;path_to_NovaScope_repository&gt;\"            # Replace &lt;path_to_NovaScope_repository&gt; with the path to the NovaScope repository\njob_dir=\"&lt;path_to_the_job_directory&gt;\"               # Replace &lt;path_to_the_job_directory&gt; with your specific job directory path\nslurm_params=\"--profile &lt;path_to_slurm_directory&gt;\"  # Replace &lt;path_to_slurm_directory&gt; with your directory of the SLURM configuration file\n\n# Execute the NovaScope pipeline\nsnakemake $slurm_params  --latency-wait 120  -s ${smk_dir}/NovaScope.smk  -d $job_dir \n</code></pre> <p>Specific examples prepared for the three datasets are provided below:</p> <ul> <li>Minimal Test Run</li> <li>Shallow Liver Section</li> <li>Deep Liver Section test runs.</li> </ul> <p>Then submit the master job through <code>sbatch</code>:</p> <pre><code>sbatch submit_HPC.job\n</code></pre>"},{"location":"getting_started/execute/#option-c-slurm-via-command-line","title":"Option C: SLURM via Command Line","text":"<p>For a small number of quick jobs, you can submit them with a single command line without submitting a master job through Slurm.</p> <p>This is similar to the local execution, but you need to specify the Slurm parameters.</p> <p>It is important to remember that if you are logged out before all jobs have been submitted to Slurm, any remaining jobs, i.e., those haven't been submitted, will not be submitted.</p> <pre><code>smk_dir=\"&lt;path_to_NovaScope_repository&gt;\"            # Replace &lt;path_to_NovaScope_repository&gt; with the path to the NovaScope repository\njob_dir=\"&lt;path_to_the_job_directory&gt;\"               # Replace &lt;path_to_the_job_directory&gt; with your specific job directory path\nslurm_params=\"--profile &lt;path_to_slurm_directory&gt;\"  # Replace &lt;path_to_slurm_directory&gt; with your directory of the SLURM configuration file\n\nsnakemake $slurm_params --latency-wait 120 -s ${smk_dir}/NovaScope.smk -d $job_dir \n</code></pre>"},{"location":"getting_started/intro/","title":"Getting Started with NovaScope","text":""},{"location":"getting_started/intro/#introduction","title":"Introduction","text":"<p>This tutorial provides an instruction of running  NovaScope based on the  three example datasets  provided with the published protocol. </p> <p>Before downloading the example datasets, you must install NovaScope in your computing environment, and  set up your own config file. </p> <p>In this tutorial, we will provide guidance on (a) how to access the  example datasets, (b) how to set up your input configuration file, and (c) how to execute the NovaScope pipeline based on the prepared input files.</p>"},{"location":"getting_started/job_config/","title":"Configuring a NovaScope Run","text":""},{"location":"getting_started/job_config/#overview","title":"Overview","text":"<p>Once you have installed NovaScope and downloaded the input data, the next step is to configure a NovaScope run. This mainly involves preparing the input configuration files (in YAML) for the run.</p>"},{"location":"getting_started/job_config/#preparing-input-config-files","title":"Preparing Input Config Files","text":"<p>The pipeline requires to have <code>config_job.yaml</code> file in the working directory (specified by <code>-d</code> or <code>--directory</code>) to specify all input files, output files, and parameters. </p> <p>For user's convenience, we provide separate example <code>config_job.yaml</code> files for the Minimal Test Run Dataset, Shallow Liver Section Dataset, and Deep Liver Section Dataset test runs are provided.  </p> <p>The details of each item specified in the <code>config_job.yaml</code> is described below:</p>"},{"location":"getting_started/job_config/#a-template-of-the-config-file","title":"A Template of the Config File","text":"<p>Below is a template of the <code>config_job.yaml</code> file.  Mandatory fields are marked as \"REQUIRED FIELD\".</p> <pre><code>## Section to Specify Input Datta\ninput:\n  flowcell: &lt;flowcell_id&gt;                       ## REQUIRED FIELD (e.g. N3-HG5MC)\n  section: &lt;section_chip_id&gt;                    ## REQUIRED FIELD (e.g. B08C)\n  species: &lt;species_info&gt;                       ## REQUIRED FIELD (e.g. \"mouse\")\n  lane: &lt;lane_id&gt;                               ## Optional. Auto-assigned based on section's last letter if absent (A-&gt;1, B-&gt;2, C-&gt;3, D-&gt;4).\n  seq1st:                                       ## 1st-seq information\n    prefix: &lt;seq1st_id&gt;                         ## Optional. Defaults to \"L{lane}\" if absent.\n    fastq: &lt;path_to_seq1st_fastq_file&gt;          ## REQUIRED FIELD\n    layout: &lt;path_to_sbcd_layout&gt;               ## Optional. Default based on section_chip_id\n  seq2nd:                                       ## 2nd-seq information\n    - prefix: &lt;seq2st_pair1_id&gt;                 ## REQUIRED FIELD - for first pair of FASTQ files\n      fastq_R1: &lt;path_to_seq2nd_pair1_fastq_Read1_file&gt; ## REQUIRED FIELD - Read 1 FASTQ file\n      fastq_R2: &lt;path_to_seq2nd_pair1_fastq_Read2_file&gt; ## REQUIRED FIELD - Read 2 FASTQ file\n    - prefix: &lt;seq2st_pair2_id&gt;                 ## Optional - if there are &gt;1 pair of FASTQs\n      fastq_R1: &lt;path_to_seq2nd_pair2_fastq_Read1_file&gt;\n      fastq_R2: &lt;path_to_seq2nd_pair2_fastq_Read2_file&gt;\n    # ... (if there are more 2nd-seq FASTQ files)\n  label: &lt;seq2nd_version&gt;                       ## Optional. A version label (e.g. v1)\n  histology: &lt;path_to_the_input_histology_file&gt; ## Optional, only if histology alignment is needed.\n\n## Output\noutput: &lt;output_directory&gt;                      ## REQUIRED FIELD (e.g. /path/to/output/directory)\nrequest:                                        \n  - &lt;required_output1&gt;                          ## REQUIRED FIELD (e.g. sge-per-section)\n  - &lt;required_output2&gt;                          ## Optionally, you can request multiple outputs\n  # ...\n\n## Environment\nenv_yml: &lt;path_to_config_env.yaml_file&gt;         ## If absent, the pipeline will check if a \"config_env.yaml\" file exists in the `info` subdirectory in the Novascope repository.\n\n## ================================================\n##\n##  Additional Fields:\n## \n##    The \"preprocess\" and \"histology\" parameters are included below, along side the default values.\n##    You only need to revise and enable the following parameters if you wish to utilize values different than the default.\n##\n## ================================================\n\n### UNCOMMENT RELEVANT LINES TO ENABLE THE ADDITIONAL PARAMETERS\n#preprocess:\n#  fastq2sbcd:\n#    format: DraI32          ## Example data uses DraI31, but DraI32 is a typical format.\n#\n#  sbcd2chip:                ## specify the parameters for sbcd2chip\n#    gap_row: 0.0517\n#    gap_col: 0.0048\n#    dup_maxnum: 1\n#    dup_maxdist: 1\n#\n#  smatch:                   ## specify the parameters for smatch\n#    skip_sbcd: 1            ## If absent, default skip_sbcd follows the fastq2sbcd format: 1 for DraI31 and 0 for DraI32.\n#    match_len: 27           ## Length of spatial barcode considered to be a perfect match.\n#\n#  align:                    ## specify the parameters for align (STARsolo)\n#    min_match_len: 30       ## A minimum number of matching bases.\n#    min_match_frac: 0.66    ## A minimum fraction of matching bases.\n#    len_sbcd: 30            ## Length of spatial barcode (in Read 1) to be copied to output FASTQ file (Read 1).\n#    len_umi: 9              ## Length of UMI barcode (in Read 2) to be copied to output FASTQ file (Read 1).\n#    len_r2: 101             ## Length of read 2 after trimming (including randomers).\n#    exist_action: overwrite ## Skip the action or overwrite the file if an intermediate or output file already exists. Options: \"skip\", and \"overwrite\".\n#    resource:               ## See 2.2.\n#      assign_type: stdin\n#      stdin:\n#        partition: standard\n#        threads: 10\n#        memory: 70000m\n#\n#  dge2sdge:                 ## specify the parameters for dge2sdge\n#    layout: null            ## If absent, the layout file in the info/assets/layout_per_section_basis/layout.1x1.tsv will be used for RGB plots.\n#\n#  gene_visual: null         ## If you have a specific set of genes to visualize, specify the path to a file containing a list of gene names (one per line) here. By default, the top five genes with the highest expression are visualized.\n#\n#  visualization:            ## specify the parameters for visualization\n#    drawxy:\n#      coord_per_pixel: 1000\n#      intensity_per_obs: 50\n#      icol_x: 3\n#      icol_y: 4\n#\n#histology:                  ## specify the parameters for histology alignment using historef\n#    resolution: 10\n#    figtype: \"hne\"          ## Options: \"hne\", \"dapi\", and \"fl\".\n</code></pre>"},{"location":"getting_started/job_config/#detailed-description-of-individual-fields","title":"Detailed Description of Individual Fields","text":""},{"location":"getting_started/job_config/#input","title":"Input","text":"<p><code>seq1st</code> </p> <p><code>prefix</code></p> <p>The <code>prefix</code> will be used to organize the 1st-seq FASTQ files. Make sure the <code>prefix</code> parameter in the corresponding flowcell is unique.  </p> <p><code>layout</code></p> <p>A file to provide the layout of tiles in a section chip with the following format. If absent, NovaScope will automatically look for the sbcd layout within the NovaScope repository at info/assets/layout_per_tile_basis, using the section chip ID for reference.</p> <pre><code>lane  tile  row  col  rowshift  colshift\n3     2556  1    1    0         0\n3     2456  2    1    0         0.1715\n</code></pre> <ul> <li>lane: Lane IDs</li> <li>tile: Tile IDs </li> <li>row &amp; col: The layout position</li> <li>rowshift &amp; colshift: The gap information</li> </ul> <p><code>seq2nd</code></p> <p>Every FASTQ pair associated with the input section chip should be supplied in <code>seq2nd</code>.  The <code>prefix</code> should be unique among all 2nd-seq FASTQ pairs, not just within this flowcell.</p>"},{"location":"getting_started/job_config/#output","title":"Output","text":"<p>The output directory will be used to organize the input files and store output files. Please see the structure directory here</p>"},{"location":"getting_started/job_config/#requests","title":"Requests","text":"<p>The pipeline interprets the requested output files via this parameter and determines which jobs need to be executed. </p> <p>Simply define the final output required, and all intermediary files contributing to this output will be automatically generated (i.e.,  the dependencies between rules). For instance, outputs from <code>\"sbcd-per-flowcell\"</code> serve as inputs for <code>\"sbcd-per-section\"</code>. Thus, by requesting <code>\"sbcd-per-section\"</code>, the pipeline will generate not only the files for <code>\"sbcd-per-section\"</code> but also those for <code>\"sbcd-per-flowcell\"</code>. For detailed insights into these dependencies, please consult the rulegraph.</p> <p>The options and corresponding output files are listed below:</p> <ul> <li><code>\"sbcd-per-flowcell\"</code>: A spatial barcode map for a flowcell organzied on a per-tile basis. Each tile has a compressed tab-delimited file for barcodes and corresponding local coordinates in the tile.</li> <li><code>\"sbcd-per-section\"</code>: A spatial barcode map for a section chip, including a compressed tab-delimited file for barcodes and corresponding global coordinates in the section chip, and an image displaying the spatial distribution of the barcodes' coordinates.</li> <li><code>\"smatch-per-section\"</code>: A compressed tab-delimited file with spatial barcodes corresponding to the 2nd-seq reads, a \"smatch\" image depicting the distribution of spatial coordinates for the matching barcodes, and a summary file of the matching results. </li> <li><code>\"align-per-section\"</code>: A BAM file accompanied by alignment summary metrics, along with spatial digital gene expression (sDGE) matrices for Gene, GeneFull, splice junctions (SJ), and Velocyto.</li> <li><code>\"sge-per-section\"</code>: An sDGE matrix, an \"sge\" image depicting the spatial alignment of transcripts, and an RGB image representing the sDGE matrix and selected genes. In the absence of specified genes of interest, the RGB image will display the top 5 genes with the highest expression levels.</li> <li><code>\"hist-per-section\"</code>: Two aligned histology files, one of which is a referenced geotiff file facilitating the coordinate transformation between the SGE matrix and the histology image. The other is a tiff file matching the dimensions of both the \"smatch\" and \"sge\" images.</li> </ul>"},{"location":"getting_started/job_config/#preprocess","title":"preprocess","text":"<p><code>align</code></p> <p><code>resource</code>: </p> <p>The <code>resource</code> parameters are only applicable for HPC users. The <code>assign_type</code> include two options: <code>\"stdin\"</code> (recommended) and <code>\"filesize\"</code>. </p> <p>If using <code>\"stdin\"</code>, define the resource parameters in the <code>stdin</code>, including <code>partition</code>, <code>threads</code>, and <code>memory</code>, to fit your case. Such resource will be used for the align step. An example is provided below. </p> <pre><code>preprocess:\n#  ...\n  align:\n#    ...\n    resource:\n      assign_type: stdin\n      stdin:\n        partition: standard\n        threads: 10\n        memory: 70000m\n</code></pre> <p>If using <code>\"filesize\"</code>, ensure to include details about the computing capabilities of all available nodes. This includes the partition name, the available number of CPUs, and the memory allocated per CPU (refer to the example provided). Resource allocation will be automatically adjusted based on the total size of the input 2nd-seq FASTQ files and the available computing resources. The preliminary strategy for resource allocation is as follows: for input 2nd-seq FASTQ files smaller than 200GB, allocate 70GB of memory for alignment processes; for file sizes ranging from 200GB to 400GB, allocate 140GB of memory; for anything larger, 330GB of memory will be designated for alignment step.</p> <pre><code>preprocess:\n#  ...\n  align:\n#    ...\n    resource:\n      assign_type: filesize\n      filesize:\n        - partition: standard\n          max_n_cpus: 20\n          mem_per_cpu: 7g\n        - partition: largemem\n          max_n_cpus: 10\n          mem_per_cpu: 25g\n</code></pre>"},{"location":"getting_started/output/","title":"Expected Output from NovaScope","text":""},{"location":"getting_started/output/#output-directory-structure","title":"Output Directory Structure","text":"<p>The directory passed through <code>output</code> paramter in the <code>config_job.yaml</code> will be organized as follows, </p> <pre><code>\u251c\u2500\u2500 align\n\u251c\u2500\u2500 histology\n\u251c\u2500\u2500 seq1st\n\u2514\u2500\u2500 seq2nd\n</code></pre>"},{"location":"getting_started/output/#seq1st","title":"seq1st","text":"<p>The seq1st directory is structured for organizing 1st sequencing FASTQ files and spatial barcode maps. It includes:</p> <ul> <li>A <code>fastqs</code> subdirectory for all input 1st sequencing FASTQ files via symlink.</li> <li>Two subdirectories for spatial barcode maps:<ul> <li><code>sbcds</code> for maps of individual tiles from the 1st sequencing,</li> <li><code>nbcds</code> for a map organized on a per-chip basis, used in later processing.</li> </ul> </li> </ul> <pre><code>\u2514\u2500\u2500 seq1st\n    \u2514\u2500\u2500 &lt;flowcell_ID&gt;\n        \u251c\u2500\u2500 fastqs\n        \u251c\u2500\u2500 nbcds\n        \u2514\u2500\u2500 sbcds\n</code></pre>"},{"location":"getting_started/output/#seq2nd","title":"seq2nd","text":"<p>The <code>seq2nd</code> directory is dedicated to managing all input 2nd sequencing FASTQ files via symlinks. The directory structure is as follows:</p> <pre><code>\u2514\u2500\u2500 seq2nd\n    \u251c\u2500\u2500 &lt;prefix1&gt;\n    |   \u251c\u2500\u2500 &lt;prefix1&gt;.R1.fastq.gz\n    |   \u2514\u2500\u2500 &lt;prefix1&gt;.R2.fastq.gz\n    \u2514\u2500\u2500 &lt;prefix2&gt;\n        \u251c\u2500\u2500 &lt;prefix2&gt;.R1.fastq.gz\n        \u2514\u2500\u2500 &lt;prefix2&gt;.R2.fastq.gz\n</code></pre>"},{"location":"getting_started/output/#histology","title":"histology","text":"<p>The <code>histology</code> directory is designated for holding all input histology files.</p>"},{"location":"getting_started/output/#align","title":"align","text":"<p>The <code>align</code> directory encompasses several subdirectories, including:  (1) <code>match</code>, which houses the outcomes of aligning second sequencing reads with spatial barcodes for the corresponding chip section;  (2) <code>bam</code>, where alignment outcomes such as the BAM file, summary metrics, and visualizations are stored;  (3) <code>sge</code>, containing a spatial gene expression (SGE) matrix and its associated visualizations;  (4) <code>histology</code>, which stores histology images aligned with the spatial coordinates of the SGE matrix.</p> <pre><code>align\n\u2514\u2500\u2500 &lt;flowcell_ID&gt;\n    \u2514\u2500\u2500 &lt;section_chip_ID&gt;\n     \u00a0\u00a0 \u251c\u2500\u2500 bam\n     \u00a0\u00a0 \u251c\u2500\u2500 histology\n     \u00a0\u00a0 \u251c\u2500\u2500 match\n     \u00a0\u00a0 \u2514\u2500\u2500 sge\n</code></pre>"},{"location":"getting_started/output/#downstream-analysis","title":"Downstream Analysis","text":"<p>The aligned sequenced reads can be directly used for tasks that require read-level information, such as allele-specific expression or somatic variant analysis. The SGE matrix can also be analyzed with many software tools, such as Latent Dirichlet Allocation (LDA) and Seurat. </p> <p>An exemplary downstream analysis is provided at NovaScope-exemplary-downstream-analysis.</p>"},{"location":"home/documentation_overview/","title":"Documentation Overview","text":"<p>The current documentation of NovaScope  include the following sections:</p> <ul> <li>Installation:<ul> <li>Requirements: Instructions on how to install necessary software tools and obtain reference datasets.</li> <li>Environment Setup: A quick guide to set up your environment YAML file.</li> <li>Slurm: (Optional) Instructions for creating a configuration file for the SLURM scheduler.</li> </ul> </li> <li>Getting Started:<ul> <li>Preparing Input: How to ready your input data and configuration file.</li> <li>Execute NovaScope: Three options to execute the pipeline.</li> <li>Output: Details on the structure of the output directory and the usage of the produced data.</li> </ul> </li> </ul>"},{"location":"home/workflow_structure/","title":"Workflow Structure","text":"<p>The workflow of NovaScope can be visualized in the following rule graph.</p> <p>Figure 2: The overall flow and dependencies between rules: Each node in the graph represents a rule within your Snakemake workflow. Each arrow among nodes stands for the rule dependency among rules, with the direction that points from prerequisite rules to a dependent rule. The prerequisite rules must be executed before the dependent rule can start.</p>"},{"location":"installation/env_setup/","title":"Setting up a Environment YAML File","text":"<p>NovaScope requires a YAML file to configure the environment. This file is used to specify the paths to the required tools, reference databases, and Python environment. To create your own <code>config_env.yaml</code> file for the environment setup, you may copy from our example available in our GitHub repository.</p> <p>Below is a brief description of all the items in the YAML file. Replace the placeholders with your specific input variables to customize it according to your needs, and prepare your own <code>config_env.yaml</code>.</p>"},{"location":"installation/env_setup/#tools","title":"Tools","text":"<p>For tools that are not explicitly defined, the pipeline will automatically check if they are installed and include them in the system path for use. This allows the pipeline to utilize these tools without needing manual configuration for each one. <pre><code>tools:\n  spatula: /path/to/spatula/bin/spatula                     ## Default: \"spatula\"\n  samtools: /path/to/samtools/samtools                      ## Default: \"samtools\"\n  star: /path/to/STAR_2_7_11b/bin/Linux_x86_64_static/STAR  ## Default: \"STAR\"\n</code></pre></p>"},{"location":"installation/env_setup/#hpc-specific-configuration","title":"HPC-specific Configuration:","text":"<p>For HPC users, use the <code>envmodules</code> section to load the required software tools as modules. If a tool is not listed in the envmodules section, the pipeline will assume it's installed system-wide. For local executions, you may remove this section if running the pipeline on your local machine.</p> <p>Please specify the version information. </p> <p><pre><code>envmodules:\n  python: \"python/&lt;version_information&gt;\"\n  gcc: \"gcc/&lt;version_information&gt;\"\n  gdal: \"gdal/&lt;version_information&gt;\"\n  imagemagick: \"imagemagick/&lt;version_information&gt;\"\n  #snakemake: \"snakemake/&lt;version_information&gt;\"\n</code></pre> * <code>python</code>: If your python environment was set up using a Python version accessed through a module, your environment depends on certain shared files from that module. Therefore, you must add the <code>python: \"python/&lt;version_information&gt;\"</code>  in the <code>envmodules</code> section to load the same module you initially used to establish your environment. But if you set up with a locally installed Python (not using <code>module load</code>), comment out or remove the module line <code>python: \"python/&lt;version_information&gt;\"</code>. * It is also feasible to use <code>envmodules</code> to load other tools, such as <code>samtools</code> instead of defining its path in <code>tools</code>.</p>"},{"location":"installation/env_setup/#reference-database","title":"Reference Database","text":"<p>Please list every reference database used for alignment here. The reference data can be obtained via the cellranger download page.</p> <p>Please ensure the reference database corresponds to the species of your input data. </p> <pre><code>ref:\n  align:\n    mouse: \"/path/to/refdata-gex-GRCm39-2024-A/star\"\n    human: \"/path/to/refdata-gex-GRCh39-2024-A/star\"\n   #...\n</code></pre>"},{"location":"installation/env_setup/#python-environment","title":"Python Environment","text":"<p>You also need to specify the path of python virtual environment by modifying the following line.</p> <pre><code>pyenv: \"/path/to/python/virtual/env\"\n</code></pre>"},{"location":"installation/requirement/","title":"Installing NovaScope","text":"<p>Installing NovaScope involves multiple steps. This document provides instructions on how to install the necessary software tools and obtain reference datasets.</p>"},{"location":"installation/requirement/#installing-snakemake","title":"Installing Snakemake","text":"<p>Snakemake orchestrates the workflow of NovaScope pipeline. We recommend installing Snakemake using conda and/or mamba. For detailed installation instructions of these tools, please refer to the official Snakemake documentation. </p>"},{"location":"installation/requirement/#checking-snakemake-installation","title":"Checking Snakemake Installation","text":"<p>If you are unsure whether Snakemake is installed in your system or not, you can check by running the following command:</p> <pre><code>snakemake --version\n</code></pre> <p>In some systems that supports <code>module</code>, you may be able to load the <code>snakemake</code> module using the following command:</p> <pre><code>## check if snakemake is available as a module\nmodule avail snakemake\n\n## load the available module (specify the version if necessary)\nmodule load snakemake\n</code></pre> <p>NovaScope has been tested for compatibility with Snakemake v7.29.0 and v8.6.0.</p>"},{"location":"installation/requirement/#installing-snakemake-using-conda-and-mamba","title":"Installing Snakemake Using Conda and Mamba","text":"<p>If you need to install Snakemake, below is a simplified sequence of instruction. Please refer to the official documentation for more detailed instructions.</p> <pre><code>## Download miniconda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n## Follow the on-screen instructions to complete the installation. \n\n## Activate the Miniconda installation:\nsource ~/.bashrc\n\n## create a new conda environment\nconda create -n snakemake-env python=3.9\n\n## activate the new environment\nconda activate snakemake-env\n\n## install mamba in the conda environment\nconda install mamba -n snakemake-env\n\n## activate the environment to ensure mamba is correctly set up\nconda activate snakemake-env\n\n## install snakemake using mamba\nmamba install snakemake\n\n## verify the installation of snakemake\nsnakemake --version\n</code></pre>"},{"location":"installation/requirement/#installing-other-dependent-tools","title":"Installing Other Dependent Tools","text":"<p>NovaScope depends on a number has been tested for compatibility with Snakemake v7.29.0 and v8.6.0.</p> <p>The dependent software tools are listed below. The versions specified for each software tool have been verified for compatibility with our pipeline, though other versions may also be compatible.</p> <ul> <li>STARsolo (v2.7.11b)</li> <li>samtools (v1.14 or v1.19)</li> <li>spatula (v0.1.0)</li> <li>Python (v3.9.12, v3.10, or v3.12.2)</li> <li>ImageMagick (7.1.0-25.lua and 7.1.1-30)</li> <li>GDAL (v3.5.1)</li> </ul> <p>We provide an example work log documenting the installation of the aforementioned software tools.</p>"},{"location":"installation/requirement/#installing-novascope_1","title":"Installing NovaScope","text":"<p>To install NovaScope, clone the repository from GitHub using the following command:</p> <pre><code>git clone https://github.com/seqscope/NovaScope.git\n</code></pre>"},{"location":"installation/requirement/#preparing-reference-genomes","title":"Preparing Reference Genomes","text":"<p>The reference genome for the species of interest must be downloaded and indexed for alignment. STARsolo accepts the reference genomes prepared by cellranger, therefore, one of the simplest way is to download the reference genome from the cellranger download page.</p> <p>The recommended reference genome for mouse is GRCm39. However, the STAR index provided with the package is outdated and will not be compatible with the latest version of STARsolo. Therefore, we recommend indexing it using the latest version of STARsolo.</p> <pre><code>## download the reference genome package\ncurl -O \"https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCm39-2024-A.tar.gz\"\n\n## uncompressed the tar file\ntar -xvf refdata-gex-GRCm39-2024-A.tar.gz\ncd refdata-gex-GRCm39-2024-A\n\n## uncompress GTF file\ngzip -d genes/genes.gtf.gz\n\n## index the reference genome\nSTARBIN=/path/to/STAR_2.7.11b/Linux_x86_64_static/STAR\n${STARBIN} --runMode genomeGenerate \\\n    --runThreadN 1 \\\n    --genomeDir ./star_2_7_11b \\\n    --genomeFastaFiles ./fasta/genome.fa \\\n    --genomeSAindexNbases 14 \\\n    --genomeChrBinNbits 18 \\\n    --genomeSAsparseD 3 \\\n    --limitGenomeGenerateRAM 17179869184 \\\n    --sjdbGTFfile ./genes/genes.gtf\n</code></pre> <p>The recommended reference genome for human is GRCh38.</p> <pre><code>## download the reference genome package\ncurl -O \"https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2024-A.tar.gz\"\n\n## uncompressed the tar file\ntar -xvf refdata-gex-GRCh38-2024-A.tar.gz\ncd refdata-gex-GRCh38-2024-A\n\n## uncompress GTF file\ngzip -d genes/genes.gtf.gz\n\n## index the reference genome\nSTARBIN=/path/to/STAR_2.7.11b/Linux_x86_64_static/STAR\n${STARBIN} --runMode genomeGenerate \\\n    --runThreadN 1 \\\n    --genomeDir ./star_2_7_11b \\\n    --genomeFastaFiles ./fasta/genome.fa \\\n    --genomeSAindexNbases 14 \\\n    --genomeChrBinNbits 18 \\\n    --genomeSAsparseD 3 \\\n    --limitGenomeGenerateRAM 17179869184 \\\n    --sjdbGTFfile ./genes/genes.gtf\n</code></pre> <p>For other species, you may follow the instructions provided by cellranger or STARsolo to prepare the reference genome.</p>"},{"location":"installation/requirement/#configuring-python-virtual-environment","title":"Configuring Python Virtual Environment","text":"<p>We recommend creating a new Python environment for NovaScope using the following steps. If you already have an existing Python environment all required packages (see pyenv_req.txt), you may skip this step. </p> <p>You may create a new Python environment using the following commands:</p> <pre><code>## set the path to the python virtual environment directory\npyenv_dir=/path/to/python/virtual/environment/directory\npyenv_name=name_of_python_venv\nsmk_dir=/path/to/the/novascope/directory\n\n## create the python virtual environment (need to be done only once)\nmkdir -p ${pyenv_dir}\ncd ${pyenv_dir}\npython -m venv ${pyenv_name}\n\n## activate the python environment (every time you want to use the environment)\nsource ${pyenv_name}/bin/activate\n\n## install the required packages (need to be done only once)\npip install -r ${smk_dir}/installation/pyenv_req.txt\n</code></pre>"},{"location":"installation/requirement/#optional-install-the-historef-package-from-the-whl-file","title":"(Optional) Install the historef Package from the whl File","text":"<p>If you want to align your histology images with the spatial gene expression data, you may install the historef package from the whl file. Below is an example instruction to download historef's latest version at document creation. To access the most recent version, please see its GitHub repository.</p> <pre><code>## activate the python environment\nsource ${pyenv_dir}/$pyenv_name/bin/activate\n\n### download the historef package\nwget -P ${smk_dir}/installation https://github.com/seqscope/historef/releases/download/v0.1.1/historef-0.1.1-py3-none-any.whl\n\n## install the historef package\npip install ${smk_dir}/installation/historef-0.1.1-py3-none-any.whl\n</code></pre>"},{"location":"installation/slurm/","title":"Snakemake with Slurm","text":"<p>It is recommended to integrate SLURM scheduler with Snakemake, which can automate the process of submitting your jobs.</p> <p>Please be aware that Snakemake introduced significant updates for cluster Configuration starting from version 8. Thus, we advise checking to verify your Snakemake version using <code>snakemake --version</code>. </p> <p>In NovaScope, we utilized a cluster configuration profile to define the details of the cluster and resources given its consistency and time-saving benefits. More details are provided below. Those files were crafted with inspiration from the smk-simple-slurm repository.</p>"},{"location":"installation/slurm/#a-cluster-configuration-file-for-snakemake-v7290","title":"A Cluster Configuration file for Snakemake v7.29.0","text":"<p>Create a <code>config.yaml</code> with the following settings. Please substitute the placeholders below, marked with &lt;&gt;, to suit your specific case. Please see our example file at slurm/v7.29.0/config.yaml. </p> <pre><code>## Cluster Configuration\n## The following setting also aids in organizing log files by creating rule-specific subdirectories within the job's log directory, each holding its own output and error files.\ncluster:\n  mkdir -p logs/{rule}/ &amp;&amp;\n  sbatch\n    --job-name={rule}_{wildcards}\n    --output=logs/{rule}/{rule}___{wildcards}___%j.out\n    --error=logs/{rule}/{rule}___{wildcards}___%j.err\n    --account={resources.account}\n    --partition={resources.partition}\n    --mem={resources.mem}\n    --time={resources.time}\n    --cpus-per-task={threads}\n    --parsable\n    --nodes={resources.nodes}\n\n## Default Resources for Jobs\ndefault-resources:\n  - partition=&lt;your_default_partition&gt;    # Replace &lt;your_default_partition&gt; with your actual partition name\n  - mem=&lt;default_memory_allocation&gt;       # Replace &lt;default_memory_allocation&gt; with memory, e.g., \"4G\"\n  - time=&lt;default_time_limit&gt;             # Replace &lt;default_time_limit&gt; with time, e.g., \"01:00:00\"\n  - nodes=&lt;default_number_of_nodes&gt;       # Replace &lt;default_number_of_nodes&gt; with nodes, e.g., \"1\"\n  - account=&lt;default_account_information&gt; # Replace &lt;default_account_information&gt; with your account info\n\n## General Snakemake Settings\njobs: &lt;max_number_of_jobs&gt;               # Replace &lt;max_number_of_jobs&gt; with your desired maximum number of concurrent jobs, e.g., 10\nlatency-wait: &lt;latency_seconds&gt;          # Replace &lt;latency_seconds&gt; with the number of seconds to wait if job output is not present, e.g., 120\nlocal-cores: &lt;local_core_count&gt;          # Replace &lt;local_core_count&gt; with the max number of cores to use locally, e.g., \"20\"\nrestart-times: &lt;restart_attempts&gt;        # Replace &lt;restart_attempts&gt; with the number of times to retry failing jobs, e.g., \"0\" for no retries\nmax-jobs-per-second: &lt;job_submission_rate&gt; # Replace &lt;job_submission_rate&gt; with the limit on how many jobs can be submitted per second, e.g., \"20\"\nkeep-going: &lt;continue_after_failure&gt;     # Replace &lt;continue_after_failure&gt; with True or False to indicate whether to continue executing other jobs after a failure\nrerun-incomplete: &lt;rerun_incomplete_jobs&gt; # Replace &lt;rerun_incomplete_jobs&gt; with True or False to decide if incomplete jobs should be rerun\nprintshellcmds: &lt;print_commands&gt;         # Replace &lt;print_commands&gt; with True or False to specify if shell commands should be printed before execution\n\n## Scheduler Settings\n#scheduler: greedy      \n\n## Conda Environment Settings\nuse-conda: &lt;True_or_False&gt;               # Enable use of Conda environments\nconda-frontend: conda                    # Specify Conda as the package manager frontend\n</code></pre>"},{"location":"installation/slurm/#a-cluster-configuration-file-for-snakemake-v860","title":"A Cluster Configuration file for Snakemake v8.6.0","text":"<p>Please first install the Snakemake executor plugin \"cluster-generic\":</p> <pre><code>pip install snakemake-executor-plugin-cluster-generic\n</code></pre> <p>Then, create the cluster configuration file with below. Please substitute the placeholders below, marked with &lt;&gt;, to suit your specific case. Please see our example file at slurm/v8.6.0/config.yaml. </p> <pre><code>executor: \"cluster-generic\"\ncluster-generic-submit-cmd: \"mkdir -p logs/{rule}/ &amp;&amp;\n  sbatch\n    --job-name={rule}_{wildcards} \\\n    --output=logs/{rule}/{rule}___{wildcards}___%j.out \\\n    --error=logs/{rule}/{rule}___{wildcards}___%j.err \\\n    --partition={resources.partition} \\\n    --mem={resources.mem} \\\n    --time={resources.time} \\\n    --cpus-per-task={threads} \\\n    --parsable \\\n    --nodes={resources.nodes} \"\n\ndefault-resources:\n  - partition=\"main\"\n  - mem=\"6500MB\"\n  - time=\"05:00:00\"\n  - nodes=1\n\n\njobs: 10                       \nlatency-wait: 120              \nlocal-cores: 20                \nrestart-times: 0               \nmax-jobs-per-second: 20\nkeep-going: True\nrerun-incomplete: True\nprintshellcmds: True\n\nsoftware-deployment-method: conda\n</code></pre>"},{"location":"installation/slurm/#additional-functions","title":"Additional functions","text":"<p>If you encounter situations where your jobs fail without any warnings, it is feasible to using a cluster status script to keep track of your jobs. See details here.</p>"}]}