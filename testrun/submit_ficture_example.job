#!/bin/bash
####  Job name
#SBATCH --account=leeju0
#SBATCH --partition=standard
#SBATCH --job-name=testrun1
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=2000m         # As a master job overseeing the status of each step, minimal memory is required.
#SBATCH --time=72:00:00             # Allocating an extended time limit is advisable due to its supervisory function, ensuring it exceeds the total process duration.
#SBATCH --mail-user=weiqiuc@umich.edu
#SBATCH --mail-type=END,FAIL,REQUEUE
#### where to write log files
#SBATCH --output=./logs/lda-%j_%x.out  # Log file path to 


# Path to the Snakemake pipeline path. Replace it with your actual directory path.
smk_file="/nfs/turbo/sph-hmkang/index/data/weiqiuc/NovaScope/NovaScope.smk"

# Path to the SLURM parameter directory, which should include a config.yaml file.
# Utilizing SLURM for job management is recommended due to the extended duration of certain FICTURE and Cartoscope steps.
# Additionally, SLURM aids in organizing log files by creating rule-specific subdirectories within the job's log directory, each holding its own output and error files.
slurm_params="--profile /nfs/turbo/sph-hmkang/index/data/weiqiuc/ScopeFlow/slurm"

# Path to your Job directory, which should contain at least a config_job.yaml file.
job_dir="/nfs/turbo/sph-hmkang/index/data/weiqiuc/NovaScope_local/testrun"

# (Optional but recommended) Give a dry run first.
#snakemake --dry-run -p --latency-wait 120 -s $smk_file --rerun-triggers mtime --rerun-incomplete -d $job_dir

# (Optional) Visualize the required steps and their dependencies.
#snakemake --rulegraph -s $smk_file --rerun-triggers mtime --rerun-incomplete -d $job_dir | dot -Tpdf > rulegraph.pdf

# Execute Snakemake pipeline.
snakemake $slurm_params --latency-wait 120 -s $smk_file --rerun-triggers mtime --rerun-incomplete -d $job_dir
